---
aliases: 
- Скрытая логика GPT
tags:
- 2025/Aug
- rchitecture/attention_emergent
- ai/computational_efficiency
- ai/architecture_comparison
- ai/prompt_optimization
- architecture/sparse_attention
author:
- Vladimir Ivanov
---
-----
##  Скрытая логика GPT
-----
Автор выдвигает гипотезу, что механизм Attention в GPT эмерджентно (самостоятельно) воспроизводит более простую логику LSTM, поскольку последовательное обновление "вектора смысла" от токена к токену вычислительно эффективнее, чем полный анализ всего контекста. 

Этот принцип, который усиливается в механизмах вроде sparse attention, подразумевает, что эффективный промптинг должен фокусироваться на манипуляции этими внутренними семантическими состояниями ("якорями"), а не на привычной человеческой логике.

---
## Zero-links
---
- [[0 Теоретические концепции и модели работы ИИ]]
- [[0 ИИ-модели и системы]]
- [[0 Фундаментальные архитектуры и их компоненты]]
- [[0 Внутренние процессы и состояния модели]]
- [[0 Механизмы внимания]]
- [[0 Техники создания подсказок (Промптинг)]]
- [[0 Внутренние процессы и состояния модели]]

---
## Links
---
- [Source](https://t.me/turboproject/1933)
- https://en.wikipedia.org/wiki/Long_short-term_memory