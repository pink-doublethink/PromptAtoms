---
aliases: 
- Исследователи с Less Wrong 
date: 27-Jun-2025
tags:
- 2025/Apr
- ИИ/ограничения/объяснимость
- Наука/методология/исследование_ИИ
- ИИ/технологии/трансформеры/архитектура
- ИИ/технологии/LLM/репрезентация_знаний
- ИИ/технологии/принципы_работы
author:
- Vladimir Ivanov
---
-----
##  Исследователи с Less Wrong 
-----
Мы не понимаем, как устроены большие ИИ вроде GPT изнутри после обучения, хотя и можем их тренировать. Это важно для понимания их работы с информацией.

Чтобы разобраться, исследуют уменьшенные модели (например, GPT-2), так как большие слишком сложны, и их внутренние "понятия" сильно отличаются от человеческих. Малые модели поддаются интерпретации.

Исследователи с "Less Wrong" изучили структуру GPT-2 (12 слоев):

- Ранние слои (0-2): Разбирают синтаксис/грамматику.
    
- Средние (3-6): Преобразуют в семантику/смысл.
    
- Поздние (7-9, особенно 9-й): Занимаются абстрактным мышлением.
    
- Последние (10-11): Формируют ответ для человека.
    

**Ключевые выводы:**

1. Большая часть работы ИИ - перевод между человеческим и внутренним языком; основные выводы делаются в немногих слоях.
    
2. Это подчеркивает критическую важность числа слоев (много нужно для перевода), объясняя многослойность современных ИИ.
    
3. Внутренние представления смысла (векторы) полифункциональны (несут несколько смыслов сразу), что делает пространство понятий ИИ трудным для человеческого понимания.

---
## Zero-links
---
- [[0 ИИ-модели и системы]]
- [[0 Внутренние процессы и состояния модели]]
- [[0 Фундаментальные архитектуры и их компоненты]]
- [[0 Прикладные подходы и кейсы]]
- [[0 Характеристики ИИ-модели]]

---
## Links
---
- [Source](https://t.me/turboproject/1550)
- https://www.lesswrong.com/posts/xmegeW5mqiBsvoaim/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don