---
aliases: 
- Как идея лингвиста стала топливом для нейросетей 
tags:
- 2025/Jun
- ai/nlp/semantic-vectors
- nlp/computational-linguistics
- ai/nlp/history
- ai/history
- ai/nlp/embeddings
- research/methodology
author:
- Vladimir Ivanov
---
-----
##  Как идея лингвиста стала топливом для нейросетей 
-----
Концепция семантического вектора, лежащая в основе современного ИИ, имеет долгую историю, которая началась задолго до эпохи нейросетей — в лингвистике.

Путь начался в 1957 году, когда Джон Фёрс сформулировал идею, что слово можно понять по его окружению. В 1970-х Джерард Солтон уже применял векторную модель для семантического поиска по документам. Конкретно для слов векторы-эмбеддинги появились в конце 1980-х в рамках латентно-семантического анализа (LSA), но эффективного способа их получения ещё не было.

Настоящий прорыв произошёл, когда мир лингвистики встретился с миром нейросетей. В 2003 году Йошуа Бенжио доказал, что нейросети могут эффективно генерировать смысловые векторы, а в 2013-м Томаш Миколов создал Word2Vec, представив технологию в её современном виде.

Таким образом, идея, десятилетиями зревшая в лингвистике, лишь недавно объединилась с вычислительной мощью нейросетей, что привело к эпохальным открытиям, свидетелями которых мы являемся.

---
## Zero-links
---
- [[0 Фундаментальные архитектуры и их компоненты]]
- [[0 Фундаментальные технологии и принципы]]
- [[0 Теоретические концепции и модели работы ИИ]]
- [[0 Структуры данных и модели знаний]]

---
## Links
---
- [Source](https://t.me/turboproject/1757)
- https://en.wikipedia.org/wiki/Distributional_semantics