---
aliases:
- Технические возможности vs. Потребность в реальных бизнес-доказательствах
tags:
- 2025/Apr
- ai/moe-finetuning/technical-analysis
- ai/moe-finetuning/business-value
- ai/moe-finetuning/technical-analysis/method-comparison
- ai/moe-finetuning/technical-analysis/scientific-basis
- concepts/stakeholder-communication
author:
- Vladimir Ivanov
---
-----
##  Тонкая настройка MoE 
-----
Дискуссия между Владимиром и Денисом касается возможности и рисков тонкой настройки (особенно методом LoRA) больших моделей с архитектурой MoE (Mixture-of-Experts), таких как DeepSeek или Mistral, в сравнении с плотными моделями (Llama, Qwen), особенно для прикладных бизнес-задач.

Владимир выражает скепсис, утверждая, что многие работы, якобы модифицирующие MoE, на деле работают с плотными моделями. Он настаивает, что реальная модификация MoE часто приводит к серьезной деградации: падению производительности (из-за избыточной активации экспертов), обвалу по бенчмаркам и даже "гибели" экспертов. 

По его мнению, LoRA для MoE несет риски и часто бессмысленна в сравнении с безопасной LoRA для плотных моделей. Владимир подчеркивает, что для бизнес-заказчика важны не технические возможности, а доказательства реального улучшения специфических отраслевых знаний, отсутствия деградации общих навыков и неувеличения нагрузки на оборудование. 

Он считает, что существующие работы не дают таких доказательств для реальных MoE в бизнес-контексте, и что для глубокого отраслевого понимания требуется именно тонкая настройка, а не только RAG, особенно для меньших моделей, используемых в корпорациях.

Денис возражает, заявляя, что LoRA для MoE технически безопасна и не вызывает описанной деградации, особенно если не модифицируется маршрутизатор экспертов. 

Он ссылается на множество академических работ, где LoRA успешно применяется к MoE без упоминаний специфических проблем деградации, предполагая, что Владимир путает это с проблемами классического файнтюнинга. 

Денис также утверждает, что для многих бизнес-задач (например, консультации по продуктам) достаточно более простых и дешевых RAG-решений, а тонкая настройка модели для таких задач неэффективна, избыточна и крайне дорога для большинства заказчиков. Он настаивает, что техническая возможность применения LoRA к MoE доказана в академической среде.

Основной спор сводится к разрыву между технической возможностью применения LoRA к MoE и наличием доказанных практических успехов для сложных бизнес-задач без негативных последствий, которые Владимир считает характерными для MoE. Также поднимается вопрос о необходимости более простого языка для бизнес-аудитории.

---
## Zero-links
---
- [[0 Профессиональные группы и роли]]
- [[0 Проблемы и вызовы в индустрии ИТ]]
- [[0 Фундаментальные технологии и принципы]]
- [[0 Фундаментальные архитектуры и их компоненты]]
- [[0 Эмерджентные явления и новое поведение]]
- [[0 Техники адаптации моделей]]

---
## Links
---
- [Source](https://t.me/turboproject/1574)
- https://rg.ru/2025/03/31/avito-predstavila-novye-nejroseti-tekstovuiu-a-vibe-i-multimodalnuiu-a-vision.html
- https://telecomdaily.ru/news/2025/04/07/avito-obognal-openai-i-google-neyroset-a-vibe-stala-luchshey-sredi-legkih-ii-modeley-na-russkom-yazyke
- 