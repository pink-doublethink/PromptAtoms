---
aliases: 
- Загадка двух КАМАЗов 
date: 29-Jun-2025
tags:
- 2025/May
- ai/transformer/architecture
- transformer/ffn
- ai/prompt-engineering/techniques
- transformer/attention-mechanism
- transformer/positional-encoding
- prompt_engineering/best-practices
author:
- Vladimir Ivanov
---
![[КАМАЗов.jpg]]

-----
##  **как прилагательные учат нейросеть считать**
-----
В архитектуре трансформеров существует неочевидная, но фундаментальная механика, позволяющая модели понимать контекст и различать идентичные объекты. Рассмотрим это на примере фразы «Красный КАМАЗ и черный КАМАЗ».

Хотя «головки внимания» находят само слово «КАМАЗ», итоговую обработку выполняет Перцептрон — двухслойная нейросеть, анализирующая «сплющенный» вектор со всех головок. Без дополнительных данных Перцептрон не смог бы понять, что в тексте два грузовика, а не один.

Ключевую роль здесь играют три компонента:

1. **Прилагательные («маркеры»):** Слова «красный» и «черный» создают уникальные сигналы в разных измерениях вектора. Они служат маркерами, которые Перцептрон легко связывает с сущностью «КАМАЗ», понимая, что речь идет о двух разных объектах.
    
2. **Softmax в механизме внимания:** Эта функция не обнуляет маловажные слова в контексте, а лишь ослабляет их вес. Благодаря этому «следы» прилагательных сохраняются в векторе и остаются видимыми для Перцептрона.
    
3. **Позиционные кодировки:** Это не просто порядковый номер слова, а сложная система, добавляющая в вектор тысячи новых измерений, которые кодируют положение слова в различных структурных группах текста. Это дает Перцептрону точную информацию о местоположении каждого из объектов.
    

Таким образом, Перцептрон, несмотря на свою простоту, обладает достаточным количеством нейронов, чтобы улавливать эти тонкие сигналы в многомерном пространстве. Он видит разные маркеры-прилагательные, соотносит их с сущностью и с помощью позиционных кодировок определяет, что это два отдельных объекта в разных частях предложения.

**Главный вывод для практики:** Подобно старому анекдоту «не жалейте заварки», промпт-инженеры не должны жалеть прилагательных. Чем больше описательных деталей вы даете модели, тем лучше Перцептрон понимает контекст, а головки внимания точнее находят корреляции.


----
![[КАМАЗо.jpg]]

---
## Zero-links
---
- [[0 Фундаментальные архитектуры и их компоненты]]
- [[0 Структурные и семантические подходы]]
- [[0 Профессиональные группы и роли]]
- [[0 Структуры данных и модели знаний]]

---
## Links
---
- [Source](https://t.me/turboproject/1656)