---
aliases: 
- Архитектор или Предсказатель, как мыслит GPT 
date: 30-Jun-2025
tags:
- 2025/Jun
- .
topics:
- .
---
![[как мыслит.jpg]]

-----
##  Архитектор или Предсказатель, как мыслит GPT 
-----
В центре дискуссии — два противоположных взгляда на механизм работы GPT.

Первая позиция утверждает, что для генерации сложных и логически связанных ответов, таких как программный код, ИИ предварительно формирует укрупнённый «план» или «граф ответа». Этот план хранится не в новых векторах, а размещается в скрытых состояниях векторов самого запроса, используя их как «семантическую коммуналку». Сторонники этой теории считают, что простой последовательный автокомплит (предсказание следующего токена) не способен обеспечить смысловую целостность на больших расстояниях, например, использовать в конце кода библиотеку, импортированную в самом начале.

Вторая позиция, опираясь на основополагающую научную работу "Attention is All You Need", полностью отрицает наличие предварительного плана. Согласно этой теории, GPT работает исключительно в режиме авторегрессии — генерирует текст токен за токеном. Возможность «смотреть вперёд» заблокирована на архитектурном уровне (через маскирование в декодере), что делает создание плана невозможным. Сложные структуры и дальние смысловые связи возникают благодаря механизму внимания (Attention), который на каждом шаге заново анализирует весь предыдущий контекст, позволяя ИИ создавать иллюзию разумного планирования через статистические закономерности.

Спор обостряется взаимными обвинениями: сторонник «плана» называет научную статью с arXiv «чушью» и «научным блогом без редактуры», а его оппонент парирует, что это самая цитируемая работа в области, и обвиняет собеседника в демагогии и неверной трактовке фундаментальных принципов.

---
## Zero-links
---
- ....

---
## Links
---
- [Source](https://t.me/turboproject/1737)